multiprocessing: False                              # Run the models and samples in parallel
path_pretrained_models: './pretrained_models'       # Path to the pretrained models
execute_code: True                                 # Execute the code after generating it. Only applies to main_batch
logfile: 'lvbench_log_2.txt'

dataset:
  data_path: $DATASETS/LVBench/raw_videos
  list_path: $DATASETS/LVBench/csvs/full.csv
  dataset_name: LVBench
  version: multiplechoice
  sample_fps: 1
  batch_size: 2 # always run with at least batch size > 1, or else it will behave differently
  # also watch it.
  max_samples: 1500
  
#   shuffle: True
#   shuffle_seed: 42
  # a single, tensorized video can be up to 25GB. i ought to have variable batch sizes... harmless if a large batch is on the same video, but go video-by-video.
  # also, as it stands, this code base re-loads the same video tensor even for different examples. so my batch size maxes out at 2.

code_gen:
    # model_class: openai
    # specific_model: o3 # can't specify temp or top_p for this one
    # prompt: ./prompts/benchmarks/lvbench.chatprompt

    model_class: openai
    specific_model: gpt-4o
    temperature: 1
    prompt: ./prompts/benchmarks/lvbench.chatprompt
    top_p: 0.9
    stop: ["\n```"]

    # model_class: codellama
    # specific_model: codellama/CodeLlama-7b-hf
    # temperature: 1
    # prompt: ./prompts/benchmarks/nextqa.completionprompt
    # max_new_tokens: 256
    # top_p: 0.9

    # model_class: deepseek
    # specific_model: deepseek-ai/DeepSeek-Coder-V2-Lite-Base
    # temperature: 1
    # prompt: ./prompts/benchmarks/nextqa.completionprompt
    # max_new_tokens: 256
    # top_p: 0.9

load_models:                                        # which code-running stage models to load
    maskrcnn: False
    clip: False
    glip: True
    owlvit: False
    tcl: False
    gpt3_qa: True
    gpt3_general: True
    depth: True
    blip: True
    saliency: False
    xvlm: True

# Configurations for code-running-stage models

detect_thresholds:                                  # Thresholds for the models that perform detection
    glip: 0.5
    maskrcnn: 0.8
    owlvit: 0.1
ratio_box_area_to_image_area: 0.0                   # Any detected patch under this size will not be returned
crop_larger_margin: True                            # Increase size of crop by 10% to include more context

verify_property:                                    # Parameters for verify_property
    model: xvlm                                     # Model to use for verify_property
    thresh_clip: 0.6
    thresh_tcl: 0.25
    thresh_xvlm: 0.6

best_match_model: xvlm                              # Which model to use for best_[image, text]_match

gpt3:
    n_votes: 1                                      # Number of tries to use for GPT-3. Use with temperature > 0
    qa_prompt: ./prompts/gpt3/gpt3_qa.txt
    guess_prompt: ./prompts/gpt3/gpt3_process_guess.txt
    temperature: 0.
    model: gpt-4o-mini

# Saving and loading parameters
save: True                                          # Save the results to a file
save_new_results: True                              # If False, overwrite the results file
results_dir: ./results/                             # Directory to save the results
use_cache: False                                     # Use cache for the models that support it (now, GPT-3)
clear_cache: False                                  # Clear stored cache
use_cached_codex: False                             # Use previously-computed Codex results
cached_codex_path: ''                               # Path to the csv results file from which to load Codex results
log_every: 20                                       # Log accuracy every n batches
wandb: False                                        # Use Weights and Biases

blip_half_precision: True                           # Use 8bit (Faster but slightly less accurate) for BLIP if True
blip_v2_model_type: blip2-flan-t5-xl               # Which model to use for BLIP-2

use_fixed_code: False                               # Use a fixed code for all samples (do not generate with Codex)
fixed_code_file: ./prompts/fixed_code/blip2.prompt  # Path to the fixed code file

select_answer_prompt: ./prompts/benchmarks/video_select_answer.chatprompt
# Global settings

# artifacts will be in ./<artifact_root>/<trial_name>
# Set trial_name to null for an autogenerated timestamp as the trial_name. Otherwise, if name is specified, we expect the directory ./<artifact_root>/<trial_name> to not exist yet.

artifact_folder: 'results'
trial_name: LVBENCH_ONLY_EXEC_30_FRAMES
wandb: False                                        # Use Weights and Biases
use_cache: False
clear_cache: False

dataset:
    # Required for all datasets
    name: LVBench

    # dataset-specific settings - whatever helps prepare the dataloader
    video_path: $DATASETS/LVBench/all_videos/
    list_path: $DATASETS/LVBench/csvs/full.csv
    sample_fps: 1
    max_num_frames: 30
    max_samples: # set to null to do the entire dataset
    shuffle: False
    shuffle_seed: 52 # will be read only if shuffle is true

    stage_generation:
      blanks: query, possible_answers, duration_s, resulting_frame_cnt 
    stage_execution:
      function_parameters: video, query, possible_answers

stage_generation:
    enabled: False

    # uncomment one model

    model:

        # category: openai
        # specific_model: o3 # can't specify temp or top_p for this one
        # prompt: ./prompts/benchmarks/lvbench.chatprompt
    
        category: openai
        batch_size: 16
        specific_model: gpt-4o
        base_prompt_path: ./prompts/stage_generation/nextqa.chatprompt
        temperature: 1
        top_p: 0.9
        stop: ["\n```"]

        # category: vllm
        # batch_size: 50
        # specific_model: codellama/CodeLlama-13b-Python-hf
        # base_prompt_path: ./prompts/benchmarks/nextqa.completionprompt
        # temperature: 1
        # max_new_tokens: 1024
        # top_p: 0.9

        # category: vllm
        # batch_size: 50
        # specific_model: deepseek-ai/DeepSeek-Coder-V2-Lite-Base
        # base_prompt_path: ./prompts/benchmarks/nextqa.completionprompt
        # temperature: 1
        # max_new_tokens: 1024
        # top_p: 0.9


stage_execution:
    enabled: True
    multiprocessing: False

    # This is REQUIRED if stage_generation is disabled and stage_execution is enabled. Otherwise, this is ignored
    stage_generation_results_path: /local3/minhtr/VIPER/VdebuggerFollowup/results/FULL_LV_BENCH_6AM_5_16/stage_generation_results.json

    timeout_s: 1800

    # model setup
    models:
        # object detection-related
        maskrcnn: 
            load: True
            batch_size: 
            processes:
                maskrcnn_find:
                    detect_threshold: 0.8
                    assign_gpu: 0
        glip:
            checkpoint_dir: ./pretrained_models/GLIP/
            size: large # large or tiny
            load: True
            batch_size: 
            processes:
                glip_find:
                    detect_threshold: 0.5

        # captioning-related
        clip:
            load: True
            batch_size: 
            processes:
                clip_verify_property:
                    threshold: 0.6
                clip_best_text_match: # no further configuration
                clip_best_image_match: # no further configuration
        tcl: 
            load: False
            checkpoint_path: ./pretrained_models/TCL/TCL_4M.pth
            batch_size: 
            processes:
                tcl_verify_property:
                    threshold: 0.25
                tcl_best_text_match: # no further configuration
        xvlm: 
            load: True
            checkpoint_path: ./pretrained_models/xvlm/retrieval_mscoco_checkpoint_9.pth
            batch_size: 
            processes:
                xvlm_verify_property:
                    threshold: 0.6
                xvlm_best_text_match: # no further configuration  

        # pure text query
        gpt:
            load: True
            batch_size: 
            n_votes: 1           # Number of tries to use for GPT-3. Use with temperature > 0
            temperature: 1
            top_p: 0.9
            model: gpt-4o-mini
            processes:
                gpt_long_qa:
                    base_prompt_path: ./prompts/stage_execution/gpt/gpt_long_qa.txt
                gpt_short_qa:
                    base_prompt_path: ./prompts/stage_execution/gpt/gpt_short_qa.txt
                gpt_select_answer:
                    base_prompt_path: ./prompts/stage_execution/gpt/gpt_select_best_answer_mcq.txt
                gpt_general: # no further configuration. prompt should be specified in args.

        # depth-related
        midas:
            load: True
            batch_size: 
            processes:
                midas_depth: # no further configuration
        
        # general image+text query-related
        blip:
            load: True
            batch_size: 

            model: blip2-flan-t5-xl
            precision: '8-bit' # 8-bit or full
            processes:
                blip_qa:
                blip_caption: # no further configuration
    
    # API configuration
    image_patch:
        crop:
            crop_larger_margin: True

        find:
            ratio_box_area_to_image_area: 0.0
            # from 'auto', 'maskrcnn_find', 'glip_find'
            use_process: auto
        verify_property:
            # from 'xvlm_verify_property', 'tcl_verify_property', 'clip_verify_property'
            use_process: xvlm_verify_property
        best_text_match:
            # from 'xvlm_best_text_match', 'tcl_best_text_match', 'clip_best_text_match'
            use_process: xvlm_best_text_match
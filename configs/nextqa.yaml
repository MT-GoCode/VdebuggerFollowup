# Global settings

# artifacts will be in ./<artifact_root>/<trial_name>
# Set trial_name to null for an autogenerated timestamp as the trial_name. Otherwise, if name is specified, we expect the directory ./<artifact_root>/<trial_name> to not exist yet.

artifact_folder: 'results'
trial_name: 5-12AM_FULL_VAL_NEXTQA
wandb: False                                        # Use Weights and Biases
use_cache: False
clear_cache: False

dataset:
    # Required for all datasets
    name: NExTQA

    # dataset-specific settings - whatever helps prepare the dataloader
    data_path: $DATASETS/NExTVideo/NExTVideo/
    list_path: $DATASETS/NExTVideo/csvs/val.csv
    version: multiplechoice
    sample_fps: 1
    max_num_frames: 30
    max_samples: 2 # set to null to do the entire dataset
    shuffle: True
    shuffle_seed: 90 # will be read only if shuffle is true
    
    stage_generation:
      blanks: query, extra_context 
    stage_execution:
      function_parameters: video, query, possible_answers

    # The dataset's __getitem__ is required to provide a dictionary, with keys that match the union of the list of strings in stage_generation_blanks and stage_execution_function_signature.

    # in this case:
    
    # stage_generation will, for every item in the dataset (i.e. every call of __getitem__) search base_prompt_path text file (specified in stage_generation.model) for $query$ and $extra_context$ in the text prompt, then replace it with __getitem__()[query] & __getitem__()[extra_context], which must both be strings. extend this as you please.

    # stage_execution will create a function signture with "video, query, possible_answers." For every item in the dataset (i.e. every call of __getitem__), it will expect a dictionary with matching keys, and pass these to the executing function.

    # finally, __getitem__, during stage 2, must also provide these keys (for evaluation): answer (string), possible_answers (list of strings). In addition, both stages, __getitem__ must provide an "id" key, which maps to an id string which should only use characters that can be put in a function name (it will be concatented onto the function name)
    
    # main.py exposes a global_context dictionary which can be used to query what stage the pipeline is in and customize __getitem__'s behavior accordingly. the main benefit is to avoid doing heavy video/image processing during stage 1
    
    # By convention, the first parameter of stage_execution_function_signature is a key which the dataloader should fill with a tensor compatible with ImagePatch or VideoSegment.
    
# batch size is a processing parameter unique to each processing component. the pipeline will attempt to use the largest batch size possible at any point.

stage_generation:
    enabled: True

    # uncomment one model

    model:

        # category: openai
        # specific_model: o3 # can't specify temp or top_p for this one
        # prompt: ./prompts/benchmarks/lvbench.chatprompt
    
        # category: openai
        # batch_size: 16
        # specific_model: gpt-4o
        # base_prompt_path: ./prompts/stage_generation/nextqa.chatprompt
        # temperature: 1
        # top_p: 0.9
        # stop: ["\n```"]

        category: vllm
        batch_size: 50
        specific_model: codellama/CodeLlama-13b-Python-hf
        base_prompt_path: ./prompts/benchmarks/nextqa.completionprompt
        temperature: 1
        max_new_tokens: 1024
        top_p: 0.9

        # category: vllm
        # batch_size: 50
        # specific_model: deepseek-ai/DeepSeek-Coder-V2-Lite-Base
        # base_prompt_path: ./prompts/benchmarks/nextqa.completionprompt
        # temperature: 1
        # max_new_tokens: 1024
        # top_p: 0.9


stage_execution:
    enabled: True
    multiprocessing: False

    # This is REQUIRED if stage_generation is disabled and stage_execution is enabled. Otherwise, this is ignored
    stage_generation_results_path: /local3/minhtr/VIPER/VdebuggerFollowup/results/5.16.2025.2.25.46AM/stage_generation_results.json

    timeout_s: 5

    # model setup
    models:
        # object detection-related
        maskrcnn: 
            load: True
            batch_size: 
            processes:
                maskrcnn_find:
                    detect_threshold: 0.8
        glip:
            checkpoint_dir: ./pretrained_models/GLIP/
            size: large # large or tiny
            load: True
            batch_size: 
            processes:
                glip_find:
                    detect_threshold: 0.5

        # captioning-related
        clip:
            load: True
            batch_size: 
            processes:
                clip_verify_property:
                    threshold: 0.6
                clip_best_text_match: # no further configuration
                clip_best_image_match: # no further configuration
        tcl: 
            load: False
            checkpoint_path: ./pretrained_models/TCL/TCL_4M.pth
            batch_size: 
            processes:
                tcl_verify_property:
                    threshold: 0.25
                tcl_best_text_match: # no further configuration
        xvlm: 
            load: True
            checkpoint_path: ./pretrained_models/xvlm/retrieval_mscoco_checkpoint_9.pth
            batch_size: 
            processes:
                xvlm_verify_property:
                    threshold: 0.6
                xvlm_best_text_match: # no further configuration  

        # pure text query
        gpt:
            load: True
            batch_size: 
            n_votes: 1           # Number of tries to use for GPT-3. Use with temperature > 0
            temperature: 1
            top_p: 0.9
            model: gpt-4o-mini
            processes:
                gpt_long_qa:
                    base_prompt_path: ./prompts/stage_execution/gpt/gpt_long_qa.txt
                gpt_short_qa:
                    base_prompt_path: ./prompts/stage_execution/gpt/gpt_short_qa.txt
                gpt_select_answer:
                    base_prompt_path: ./prompts/stage_execution/gpt/gpt_select_best_answer_mcq.txt
                gpt_general: # no further configuration. prompt should be specified in args.

        # depth-related
        midas:
            load: True
            batch_size: 
            processes:
                midas_depth: # no further configuration
        
        # general image+text query-related
        blip:
            load: True
            batch_size: 

            model: blip2-flan-t5-xl
            precision: '8-bit' # 8-bit or full
            processes:
                blip_qa: # no further configuration
                blip_caption: # no further configuration
    
    # API configuration
    image_patch:
        crop:
            crop_larger_margin: True

        find:
            ratio_box_area_to_image_area: 0.0
            # from 'auto', 'maskrcnn_find', 'glip_find'
            use_process: auto
        verify_property:
            # from 'xvlm_verify_property', 'tcl_verify_property', 'clip_verify_property'
            use_process: xvlm_verify_property
        best_text_match:
            # from 'xvlm_best_text_match', 'tcl_best_text_match', 'clip_best_text_match'
            use_process: xvlm_best_text_match
    
    # all other APIs have no configuration and will use the only process available for that task.
    # ImagePatch.exists -> ImagePatch.simple_query
    # ImagePatch.simple_query -> blip_query
    # ImagePatch.compute_depth -> midas_depth
    # ImagePatch.crop -> needs no model
    # ImagePatch.overlaps_with -> needs no model
    # Non-member ImagePatch APIs:
        # best_image_match -> clip_compare
        # distance -> needs no model
        # bool_to_yesno -> needs no model
        # llm_query(long_answer = True) -> gpt_general
        # llm_query(long_answer = False) -> gpt_qa
        # coerce_to_numeric -> needs no model
    
    # VideoSegment
    # VideoSegment.frame_iterator -> needs no model
    # VideoSegment.trim -> needs no model
    # VideoSegment.select_answer -> gpt_video_select_answer

# note: exclude owlvit+saliency and also auto-log every 1.
# Global settings

# artifacts will be in ./<artifact_root>/<trial_name>
# Set trial_name to null for an autogenerated timestamp as the trial_name. Otherwise, if name is specified, we expect the directory ./<artifact_root>/<trial_name> to not exist yet.

artifact_folder: 'results'
trial_name: 50_NEXTQA_SINGLE_POST_EDITS
wandb: False                                        # Use Weights and Biases
use_cache: False
clear_cache: False

dataset:
    # Required for all datasets
    name: NExTQA

    # dataset-specific settings - whatever helps prepare the dataloader
    data_path: $DATASETS/NExTVideo/
    list_path: $DATASETS/NExTVideo/csvs/val.csv
    version: multiplechoice
    
    # Note, if you plan to use some trial's stage one generation results and only run stage two execution, these settings must match to ensure it selects the same subset of the dataset, in the same order
    max_samples: 50
    shuffle: True
    shuffle_seed: 49 # will be read only if shuffle is true

    stage_generation:
      blanks: query, extra_context 


    stage_execution:
      function_parameters: video, query, possible_answers

      # dataset-specific settings - whatever helps prepare the dataloader
      sample_fps: 1
      max_num_frames: 30 

    # The dataset's __getitem__ is required to provide a dictionary, with keys that match the union of the list of strings in stage_generation_blanks and stage_execution_function_signature.

    # in this case:
    
    # stage_generation will, for every item in the dataset (i.e. every call of __getitem__) search base_prompt_path text file (specified in stage_generation.model) for $query$ and $extra_context$ in the text prompt, then replace it with __getitem__()[query] & __getitem__()[extra_context], which must both be strings. extend this as you please.

    # stage_execution will create a function signture with "video, query, possible_answers." For every item in the dataset (i.e. every call of __getitem__), it will expect a dictionary with matching keys, and pass these to the executing function.

    # finally, __getitem__, during stage 2, must also provide these keys (for evaluation): answer (string), possible_answers (list of strings). In addition, both stages, __getitem__ must provide an "id" key, which maps to an id string which should only use characters that can be put in a function name (it will be concatented onto the function name)
    
    # main.py exposes a global_context dictionary which can be used to query what stage the pipeline is in and customize __getitem__'s behavior accordingly. the main benefit is to avoid doing heavy video/image processing during stage 1
    
    # By convention, the first parameter of stage_execution_function_signature is a key which the dataloader should fill with a tensor compatible with ImagePatch or VideoSegment.
    
# batch size is a processing parameter unique to each processing component. the pipeline will attempt to use the largest batch size possible at any point.

stage_generation:
    enabled: False

    # uncomment one model

    model:

        # category: openai
        # specific_model: o3 # can't specify temp or top_p for this one
        # prompt: ./prompts/benchmarks/lvbench.chatprompt
    
        # category: openai
        # batch_size: 16
        # specific_model: gpt-4o
        # base_prompt_path: ./prompts/stage_generation/nextqa.chatprompt
        # temperature: 1
        # top_p: 0.9
        # stop: ["\n```"]

        category: vllm
        batch_size: 32
        specific_model: codellama/CodeLlama-13b-Python-hf
        base_prompt_path: ./prompts/stage_generation/nextqa.completionprompt
        temperature: 1
        max_new_tokens: 1024
        top_p: 0.9

        # category: vllm
        # batch_size: 50
        # specific_model: deepseek-ai/DeepSeek-Coder-V2-Lite-Base
        # base_prompt_path: ./prompts/benchmarks/nextqa.completionprompt
        # temperature: 1
        # max_new_tokens: 1024
        # top_p: 0.9


stage_execution:
    enabled: True
    multiprocessing:
        use: False
        num_workers: 20
    # the max effective # of workers is probably python -c "import os; print(os.cpu_count())" minus number of processes, which you have one of each
    

    # This is REQUIRED if stage_generation is disabled and stage_execution is enabled. Otherwise, this is ignored
    stage_generation_results_path: /local/minhtr/VIPER/VdebuggerFollowup/results/50_CODEGEN_NEXTQA/stage_generation_results.json

    timeout_s: 1800 # if multiprocessing, recommended to set this higher or not at all... there's wait times!

    # model setup
    # You can assign every process to a certain GPU. A GPU assignment per process is expected if the model, defined in code, expects one. A copy of the model will be instantiated on the GPU specified for every process, even if it's on the same GPU. 
    # deduplication (general deduplication in both the single and multiprocessing case) is possible, but i have to code that and that's tedious. Plus I got 8 GPUs
    # batch size must be specified and will be used for batch-only models
    # BTW, multi-GPU initialization will be done either for single or multiprocessing. The only difference is in multiprocessing we'll have separate OS processes per model process too
    models:
        # object detection-related
        owlv2:
            load: True
            batch_size: 
            processes:
                owlv2_find: # no further configuration
                    detect_threshold: 0.5
                    gpu: 0

        # captioning-related
        clip:
            load: True
            batch_size: 
            processes:
                # These aren't used; XVLM preferred
                # clip_verify_property:
                #     threshold: 0.6
                # clip_best_text_match: # no further configuration
                clip_best_image_match: # no further configuration
                    gpu: 1

        xvlm: 
            load: True
            checkpoint_path: ./pretrained_models/xvlm/retrieval_mscoco_checkpoint_9.pth
            batch_size: 
            processes:
                xvlm_verify_property:
                    threshold: 0.6
                    gpu: 1
                xvlm_best_text_match: # no further configuration  
                    gpu: 1

        # pure text query
        gpt:
            load: True
            batch_size: 
            n_votes: 1           # Number of tries to use for GPT-3. Use with temperature > 0
            temperature: 1
            top_p: 0.9
            model: gpt-4o-mini
            processes:
                # No GPU needed
                gpt_long_qa:
                    base_prompt_path: ./prompts/stage_execution/gpt/gpt_long_qa.txt
                gpt_short_qa:
                    base_prompt_path: ./prompts/stage_execution/gpt/gpt_short_qa.txt
                gpt_select_answer:
                    base_prompt_path: ./prompts/stage_execution/gpt/gpt_select_best_answer_mcq.txt
                gpt_general: # no further configuration. prompt should be specified in args.
        
        # general image+text query-related
        blip:
            load: True
            batch_size: 32
            model: blip2-flan-t5-xl
            precision: '8-bit' # 8-bit or full
            processes:
                blip_qa:
                    gpu: 2
                blip_caption:
                    gpu: 3

        # Unused

        # depth-related
        # midas:
        #     load: True
        #     batch_size: 
        #     deduplicate: False
        #     processes:
        #         midas_depth: # no further configuration

        # tcl: 
        #     load: False
        #     checkpoint_path: ./pretrained_models/TCL/TCL_4M.pth
        #     batch_size: 
        #     deduplicate: False
        #     processes:
        #         # Not used; XVLM preferred
        #         # tcl_verify_property:
        #         #     threshold: 0.25
        #         # tcl_best_text_match: # no further configuration

        # maskrcnn: 
        #     load: True
        #     deduplicate_same_gpu: False
        #     processes:
        #         maskrcnn_find:
        #             detect_threshold: 0.8
        #             gpu: 0
        # glip:
        #     checkpoint_dir: ./pretrained_models/GLIP/
        #     size: large # large or tiny
        #     load: True
        #     batch_size: 
        #     deduplicate_same_gpu: False
        #     processes:
        #         glip_find:
        #             detect_threshold: 0.5
        #             gpu: 1
    
    # API configuration
    image_patch:
        crop:
            crop_larger_margin: True

        find:
            ratio_box_area_to_image_area: 0.0
            # from 'maskrcnn_find', 'glip_find'
            use_process: 'owlv2_find'
        verify_property:
            # from 'xvlm_verify_property', 'tcl_verify_property', 'clip_verify_property'
            use_process: xvlm_verify_property
        best_text_match:
            # from 'xvlm_best_text_match', 'tcl_best_text_match', 'clip_best_text_match'
            use_process: xvlm_best_text_match
    
    # all other APIs have no configuration and will use the only process available for that task.
    # ImagePatch.exists -> ImagePatch.simple_query
    # ImagePatch.simple_query -> blip_query
    # ImagePatch.compute_depth -> midas_depth
    # ImagePatch.crop -> needs no model
    # ImagePatch.overlaps_with -> needs no model
    # Non-member ImagePatch APIs:
        # best_image_match -> clip_compare
        # distance -> needs no model
        # bool_to_yesno -> needs no model
        # llm_query(long_answer = True) -> gpt_general
        # llm_query(long_answer = False) -> gpt_qa
        # coerce_to_numeric -> needs no model
    
    # VideoSegment
    # VideoSegment.frame_iterator -> needs no model
    # VideoSegment.trim -> needs no model
    # VideoSegment.select_answer -> gpt_video_select_answer

# note: exclude owlvit+saliency and also auto-log every 1.